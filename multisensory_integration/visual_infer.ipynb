{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset,DataLoader,sampler\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Goal of the module\n",
    "Training the visual inference model. Use the visual data to predict the position of the hand.\n",
    "\n",
    "### Input\n",
    "image\n",
    "\n",
    "### Output\n",
    "spatial_coding of the hand position\n",
    "\n",
    "### Model structure\n",
    "input-->Conv-->MaxPooling-->Conv-->MaxPooling-->FC-->FC-->Softmax\n",
    "### To do\n",
    "1. Build the visual dataset</br>\n",
    "a. Convert the videos to the images </br>\n",
    "b. Learn to build the Pytorch image dataset.\n",
    "2. Train the visual inference network to make it predict the hand position.\n",
    "3. Build a multisensory dataset</br>\n",
    "a. Align the visual and prioproceptive data.</br>\n",
    "b. Merge them in a single dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'elbow_angle', 'elbow_x', 'elbow_y', 'frame_idx', 'hand_x', 'hand_y', 'shoulder_angle', 'target_x', 'video_idx']\n",
      "       Unnamed: 0  elbow_angle  elbow_x  elbow_y  frame_idx  hand_x  hand_y  \\\n",
      "0               0       113.81     0.44     2.97          0   -1.25    2.97   \n",
      "1               1       113.81     0.29     2.99          1   -1.46    2.99   \n",
      "2               2       113.81     0.44     2.97          2   -1.25    2.97   \n",
      "3               3       116.81     0.44     2.97          3   -1.19    2.97   \n",
      "4               4       113.81     0.44     2.97          4   -1.25    2.97   \n",
      "5               5       113.81     0.60     2.94          5   -1.03    2.94   \n",
      "6               6       113.81     0.44     2.97          6   -1.25    2.97   \n",
      "7               7       110.81     0.44     2.97          7   -1.30    2.97   \n",
      "8               8       107.81     0.44     2.97          8   -1.35    2.97   \n",
      "9               9       104.81     0.44     2.97          9   -1.39    2.97   \n",
      "10             10       104.81     0.29     2.99         10   -1.59    2.99   \n",
      "11             11       101.81     0.29     2.99         11   -1.62    2.99   \n",
      "12             12       104.81     0.29     2.99         12   -1.59    2.99   \n",
      "13             13       101.81     0.29     2.99         13   -1.62    2.99   \n",
      "14             14        98.81     0.29     2.99         14   -1.65    2.99   \n",
      "15             15       101.81     0.29     2.99         15   -1.62    2.99   \n",
      "16             16       101.81     0.44     2.97         16   -1.43    2.97   \n",
      "17             17       101.81     0.29     2.99         17   -1.62    2.99   \n",
      "18             18        98.81     0.29     2.99         18   -1.65    2.99   \n",
      "19             19       101.81     0.29     2.99         19   -1.62    2.99   \n",
      "20             20       101.81     0.13     3.00         20   -1.81    3.00   \n",
      "21             21       101.81    -0.03     3.00         21   -1.99    3.00   \n",
      "22             22       101.81     0.13     3.00         22   -1.81    3.00   \n",
      "23             23       101.81    -0.03     3.00         23   -1.99    3.00   \n",
      "24             24       101.81    -0.18     2.99         24   -2.16    2.99   \n",
      "25             25        98.81    -0.18     2.99         25   -2.17    2.99   \n",
      "26             26        98.81    -0.34     2.98         26   -2.34    2.98   \n",
      "27             27        55.62    -1.03     2.82          0   -2.19    2.82   \n",
      "28             28        58.62    -1.03     2.82          1   -2.27    2.82   \n",
      "29             29        58.62    -1.17     2.76          2   -2.34    2.76   \n",
      "...           ...          ...      ...      ...        ...     ...     ...   \n",
      "10688       10688        87.01    -1.88     2.33        129   -3.37    2.33   \n",
      "10689       10689        87.01    -2.00     2.23        130   -3.42    2.23   \n",
      "10690       10690        87.01    -2.12     2.12        131   -3.46    2.12   \n",
      "10691       10691        84.01    -2.12     2.12        132   -3.38    2.12   \n",
      "10692       10692        87.01    -2.12     2.12        133   -3.46    2.12   \n",
      "10693       10693        87.01    -2.00     2.23        134   -3.42    2.23   \n",
      "10694       10694        87.01    -2.12     2.12        135   -3.46    2.12   \n",
      "10695       10695        90.01    -2.12     2.12        136   -3.53    2.12   \n",
      "10696       10696        87.01    -2.12     2.12        137   -3.46    2.12   \n",
      "10697       10697        87.01    -2.00     2.23        138   -3.42    2.23   \n",
      "10698       10698        90.01    -2.00     2.23        139   -3.49    2.23   \n",
      "10699       10699        93.01    -2.00     2.23        140   -3.56    2.23   \n",
      "10700       10700        93.01    -1.88     2.33        141   -3.50    2.33   \n",
      "10701       10701        93.01    -1.76     2.43        142   -3.44    2.43   \n",
      "10702       10702        93.01    -1.63     2.52        143   -3.36    2.52   \n",
      "10703       10703        93.01    -1.50     2.60        144   -3.28    2.60   \n",
      "10704       10704        90.01    -1.50     2.60        145   -3.23    2.60   \n",
      "10705       10705        93.01    -1.50     2.60        146   -3.28    2.60   \n",
      "10706       10706        90.01    -1.50     2.60        147   -3.23    2.60   \n",
      "10707       10707        90.01    -1.36     2.68        148   -3.14    2.68   \n",
      "10708       10708        90.01    -1.22     2.74        149   -3.04    2.74   \n",
      "10709       10709        87.01    -1.22     2.74        150   -3.00    2.74   \n",
      "10710       10710        87.01    -1.07     2.80        151   -2.90    2.80   \n",
      "10711       10711        87.01    -0.92     2.85        152   -2.79    2.85   \n",
      "10712       10712        87.01    -0.77     2.90        153   -2.67    2.90   \n",
      "10713       10713        87.01    -0.62     2.94        154   -2.55    2.94   \n",
      "10714       10714        87.01    -0.46     2.96        155   -2.42    2.96   \n",
      "10715       10715        87.01    -0.31     2.98        156   -2.28    2.98   \n",
      "10716       10716        87.01    -0.15     3.00        157   -2.14    3.00   \n",
      "10717       10717        87.01     0.01     3.00        158   -1.99    3.00   \n",
      "\n",
      "       shoulder_angle  target_x  video_idx  label  label1  \n",
      "0               81.50     -2.69          0      3      -1  \n",
      "1               84.50     -2.69          0      3      -1  \n",
      "2               81.50     -2.69          0      3      -1  \n",
      "3               81.50     -2.69          0      3      -1  \n",
      "4               81.50     -2.69          0      3      -1  \n",
      "5               78.50     -2.69          0      3      -1  \n",
      "6               81.50     -2.69          0      3      -1  \n",
      "7               81.50     -2.69          0      3      -1  \n",
      "8               81.50     -2.69          0      3      -1  \n",
      "9               81.50     -2.69          0      3      -1  \n",
      "10              84.50     -2.69          0      2      -2  \n",
      "11              84.50     -2.69          0      2      -2  \n",
      "12              84.50     -2.69          0      2      -2  \n",
      "13              84.50     -2.69          0      2      -2  \n",
      "14              84.50     -2.69          0      2      -2  \n",
      "15              84.50     -2.69          0      2      -2  \n",
      "16              81.50     -2.69          0      3      -1  \n",
      "17              84.50     -2.69          0      2      -2  \n",
      "18              84.50     -2.69          0      2      -2  \n",
      "19              84.50     -2.69          0      2      -2  \n",
      "20              87.50     -2.69          0      2      -2  \n",
      "21              90.50     -2.69          0      2      -2  \n",
      "22              87.50     -2.69          0      2      -2  \n",
      "23              90.50     -2.69          0      2      -2  \n",
      "24              93.50     -2.69          0      2      -2  \n",
      "25              93.50     -2.69          0      2      -2  \n",
      "26              96.50     -2.69          0      2      -2  \n",
      "27             109.99      0.19          1      2      -2  \n",
      "28             109.99      0.19          1      2      -2  \n",
      "29             112.99      0.19          1      2      -2  \n",
      "...               ...       ...        ...    ...     ...  \n",
      "10688          128.90     -1.56         49      1      -3  \n",
      "10689          131.90     -1.56         49      1      -3  \n",
      "10690          134.90     -1.56         49      1      -3  \n",
      "10691          134.90     -1.56         49      1      -3  \n",
      "10692          134.90     -1.56         49      1      -3  \n",
      "10693          131.90     -1.56         49      1      -3  \n",
      "10694          134.90     -1.56         49      1      -3  \n",
      "10695          134.90     -1.56         49      0      -4  \n",
      "10696          134.90     -1.56         49      1      -3  \n",
      "10697          131.90     -1.56         49      1      -3  \n",
      "10698          131.90     -1.56         49      1      -3  \n",
      "10699          131.90     -1.56         49      0      -4  \n",
      "10700          128.90     -1.56         49      0      -4  \n",
      "10701          125.90     -1.56         49      1      -3  \n",
      "10702          122.90     -1.56         49      1      -3  \n",
      "10703          119.90     -1.56         49      1      -3  \n",
      "10704          119.90     -1.56         49      1      -3  \n",
      "10705          119.90     -1.56         49      1      -3  \n",
      "10706          119.90     -1.56         49      1      -3  \n",
      "10707          116.90     -1.56         49      1      -3  \n",
      "10708          113.90     -1.56         49      1      -3  \n",
      "10709          113.90     -1.56         49      1      -3  \n",
      "10710          110.90     -1.56         49      1      -3  \n",
      "10711          107.90     -1.56         49      1      -3  \n",
      "10712          104.90     -1.56         49      1      -3  \n",
      "10713          101.90     -1.56         49      1      -3  \n",
      "10714           98.90     -1.56         49      2      -2  \n",
      "10715           95.90     -1.56         49      2      -2  \n",
      "10716           92.90     -1.56         49      2      -2  \n",
      "10717           89.90     -1.56         49      2      -2  \n",
      "\n",
      "[10718 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "src_path = '/Users/Jipeng/PycharmProjects/simulated_multisensory_integration/data/'\n",
    "file_name = 'simulated_data.csv'\n",
    "df = pd.read_csv(src_path+file_name)\n",
    "\n",
    "loc_map = range(-4,5)\n",
    "def label_function(x):\n",
    "    loc_map = range(-4,5)\n",
    "    return loc_map.index(x)\n",
    "\n",
    "feature_names = df.columns.tolist()\n",
    "print feature_names\n",
    "\n",
    "label = np.around(df['hand_x'])\n",
    "df['label'] = np.around(df['hand_x'])\n",
    "df['label1'] = df['label'].astype(np.int)\n",
    "df['label'] = df['label1'].apply(label_function)\n",
    "print (df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,dataframe,image_root,transform=None):\n",
    "        self.dataset = dataframe\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, idx):\n",
    "        video_idx = self.dataset.loc[idx,'video_idx']\n",
    "        frame_idx = self.dataset.loc[idx,'frame_idx']\n",
    "        image_path = self.image_root + str(video_idx) + '/mask%s.jpg'%frame_idx\n",
    "        label = self.dataset.loc[idx,'label']\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "image_src_path = '/Users/Jipeng/PycharmProjects/simulated_multisensory_integration/data/images/mask/'\n",
    "transform = transforms.ToTensor()\n",
    "image_dataset = ImageDataset(df,image_src_path,transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def train_test_loader(train_ratio,dataset):\n",
    "    n_data = len(dataset)\n",
    "    split = int(n_data*train_ratio)\n",
    "\n",
    "    indices = list(range(n_data))\n",
    "    train_sampler = sampler.SubsetRandomSampler(indices[:split])\n",
    "    test_sampler = sampler.SubsetRandomSampler(indices[split:])\n",
    "\n",
    "    train_loader = DataLoader(dataset, sampler=train_sampler, shuffle=False, batch_size=4)\n",
    "    test_loader = DataLoader(dataset, sampler=test_sampler, shuffle=False, batch_size=4)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "train_loader, test_loader = train_test_loader(0.7,image_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        # Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(3,6,10)\n",
    "        self.pool = nn.MaxPool2d(10,10)\n",
    "        self.conv2 = nn.Conv2d(6,16,10)\n",
    "        self.fc1 = nn.Linear(960/4,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,len(loc_map))\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1,960/4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 2.231\n",
      "[1,    40] loss: 2.207\n",
      "[1,    60] loss: 2.174\n",
      "[1,    80] loss: 2.119\n",
      "[1,   100] loss: 2.111\n",
      "[1,   120] loss: 2.073\n",
      "[1,   140] loss: 2.044\n",
      "[1,   160] loss: 2.049\n",
      "[1,   180] loss: 1.999\n",
      "[1,   200] loss: 1.970\n",
      "[1,   220] loss: 1.881\n",
      "[1,   240] loss: 1.876\n",
      "[1,   260] loss: 1.882\n",
      "[1,   280] loss: 1.887\n",
      "[1,   300] loss: 1.702\n",
      "[1,   320] loss: 1.760\n",
      "[1,   340] loss: 1.700\n",
      "[1,   360] loss: 1.530\n",
      "[1,   380] loss: 1.580\n",
      "[1,   400] loss: 1.395\n",
      "[1,   420] loss: 1.462\n",
      "[1,   440] loss: 1.298\n",
      "[1,   460] loss: 1.137\n",
      "[1,   480] loss: 1.147\n",
      "[1,   500] loss: 0.798\n",
      "[1,   520] loss: 0.758\n",
      "[1,   540] loss: 0.453\n",
      "[1,   560] loss: 0.471\n",
      "[1,   580] loss: 0.350\n",
      "[1,   600] loss: 0.466\n",
      "[1,   620] loss: 0.246\n",
      "[1,   640] loss: 0.295\n",
      "[1,   660] loss: 0.252\n",
      "[1,   680] loss: 0.321\n",
      "[1,   700] loss: 0.214\n",
      "[1,   720] loss: 0.208\n",
      "[1,   740] loss: 0.180\n",
      "[1,   760] loss: 0.183\n",
      "[1,   780] loss: 0.121\n",
      "[1,   800] loss: 0.137\n",
      "[1,   820] loss: 0.171\n",
      "[1,   840] loss: 0.087\n",
      "[1,   860] loss: 0.146\n",
      "[1,   880] loss: 0.113\n",
      "[1,   900] loss: 0.206\n",
      "[1,   920] loss: 0.093\n",
      "[1,   940] loss: 0.130\n",
      "[1,   960] loss: 0.105\n",
      "[1,   980] loss: 0.158\n",
      "[1,  1000] loss: 0.098\n",
      "[1,  1020] loss: 0.066\n",
      "[1,  1040] loss: 0.074\n",
      "[1,  1060] loss: 0.185\n",
      "[1,  1080] loss: 0.050\n",
      "[1,  1100] loss: 0.089\n",
      "[1,  1120] loss: 0.112\n",
      "[1,  1140] loss: 0.078\n",
      "[1,  1160] loss: 0.051\n",
      "[1,  1180] loss: 0.061\n",
      "[1,  1200] loss: 0.114\n",
      "[1,  1220] loss: 0.058\n",
      "[1,  1240] loss: 0.100\n",
      "[1,  1260] loss: 0.056\n",
      "[1,  1280] loss: 0.093\n",
      "[1,  1300] loss: 0.066\n",
      "[1,  1320] loss: 0.059\n",
      "[1,  1340] loss: 0.053\n",
      "[1,  1360] loss: 0.109\n",
      "[1,  1380] loss: 0.051\n",
      "[1,  1400] loss: 0.076\n",
      "[1,  1420] loss: 0.047\n",
      "[1,  1440] loss: 0.067\n",
      "[1,  1460] loss: 0.071\n",
      "[1,  1480] loss: 0.055\n",
      "[1,  1500] loss: 0.052\n",
      "[1,  1520] loss: 0.038\n",
      "[1,  1540] loss: 0.117\n",
      "[1,  1560] loss: 0.110\n",
      "[1,  1580] loss: 0.046\n",
      "[1,  1600] loss: 0.023\n",
      "[1,  1620] loss: 0.071\n",
      "[1,  1640] loss: 0.031\n",
      "[1,  1660] loss: 0.100\n",
      "[1,  1680] loss: 0.072\n",
      "[1,  1700] loss: 0.081\n",
      "[1,  1720] loss: 0.026\n",
      "[1,  1740] loss: 0.022\n",
      "[1,  1760] loss: 0.079\n",
      "[1,  1780] loss: 0.031\n",
      "[1,  1800] loss: 0.015\n",
      "[1,  1820] loss: 0.039\n",
      "[1,  1840] loss: 0.032\n",
      "[1,  1860] loss: 0.025\n",
      "Finished Training\n",
      "[2,    20] loss: 0.065\n",
      "[2,    40] loss: 0.179\n",
      "[2,    60] loss: 0.063\n",
      "[2,    80] loss: 0.080\n",
      "[2,   100] loss: 0.028\n",
      "[2,   120] loss: 0.125\n",
      "[2,   140] loss: 0.117\n",
      "[2,   160] loss: 0.098\n",
      "[2,   180] loss: 0.040\n",
      "[2,   200] loss: 0.063\n",
      "[2,   220] loss: 0.029\n",
      "[2,   240] loss: 0.017\n",
      "[2,   260] loss: 0.034\n",
      "[2,   280] loss: 0.035\n",
      "[2,   300] loss: 0.024\n",
      "[2,   320] loss: 0.047\n",
      "[2,   340] loss: 0.198\n",
      "[2,   360] loss: 0.009\n",
      "[2,   380] loss: 0.042\n",
      "[2,   400] loss: 0.042\n",
      "[2,   420] loss: 0.041\n",
      "[2,   440] loss: 0.016\n",
      "[2,   460] loss: 0.067\n",
      "[2,   480] loss: 0.073\n",
      "[2,   500] loss: 0.088\n",
      "[2,   520] loss: 0.053\n",
      "[2,   540] loss: 0.061\n",
      "[2,   560] loss: 0.096\n",
      "[2,   580] loss: 0.086\n",
      "[2,   600] loss: 0.020\n",
      "[2,   620] loss: 0.035\n",
      "[2,   640] loss: 0.016\n",
      "[2,   660] loss: 0.046\n",
      "[2,   680] loss: 0.043\n",
      "[2,   700] loss: 0.059\n",
      "[2,   720] loss: 0.017\n",
      "[2,   740] loss: 0.006\n",
      "[2,   760] loss: 0.032\n",
      "[2,   780] loss: 0.049\n",
      "[2,   800] loss: 0.052\n",
      "[2,   820] loss: 0.070\n",
      "[2,   840] loss: 0.027\n",
      "[2,   860] loss: 0.047\n",
      "[2,   880] loss: 0.079\n",
      "[2,   900] loss: 0.160\n",
      "[2,   920] loss: 0.255\n",
      "[2,   940] loss: 0.054\n",
      "[2,   960] loss: 0.032\n",
      "[2,   980] loss: 0.102\n",
      "[2,  1000] loss: 0.055\n",
      "[2,  1020] loss: 0.044\n",
      "[2,  1040] loss: 0.043\n",
      "[2,  1060] loss: 0.043\n",
      "[2,  1080] loss: 0.067\n",
      "[2,  1100] loss: 0.010\n",
      "[2,  1120] loss: 0.046\n",
      "[2,  1140] loss: 0.188\n",
      "[2,  1160] loss: 0.012\n",
      "[2,  1180] loss: 0.019\n",
      "[2,  1200] loss: 0.045\n",
      "[2,  1220] loss: 0.025\n",
      "[2,  1240] loss: 0.034\n",
      "[2,  1260] loss: 0.073\n",
      "[2,  1280] loss: 0.009\n",
      "[2,  1300] loss: 0.026\n",
      "[2,  1320] loss: 0.032\n",
      "[2,  1340] loss: 0.015\n",
      "[2,  1360] loss: 0.018\n",
      "[2,  1380] loss: 0.022\n",
      "[2,  1400] loss: 0.031\n",
      "[2,  1420] loss: 0.009\n",
      "[2,  1440] loss: 0.013\n",
      "[2,  1460] loss: 0.016\n",
      "[2,  1480] loss: 0.018\n",
      "[2,  1500] loss: 0.025\n",
      "[2,  1520] loss: 0.003\n",
      "[2,  1540] loss: 0.012\n",
      "[2,  1560] loss: 0.043\n",
      "[2,  1580] loss: 0.017\n",
      "[2,  1600] loss: 0.096\n",
      "[2,  1620] loss: 0.025\n",
      "[2,  1640] loss: 0.043\n",
      "[2,  1660] loss: 0.161\n",
      "[2,  1680] loss: 0.104\n",
      "[2,  1700] loss: 0.041\n",
      "[2,  1720] loss: 0.053\n",
      "[2,  1740] loss: 0.102\n",
      "[2,  1760] loss: 0.089\n",
      "[2,  1780] loss: 0.035\n",
      "[2,  1800] loss: 0.030\n",
      "[2,  1820] loss: 0.074\n",
      "[2,  1840] loss: 0.037\n",
      "[2,  1860] loss: 0.030\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "test = 0\n",
    "for i, data in enumerate(train_loader):\n",
    "    image,label = data\n",
    "    #print (image,label)\n",
    "    if test == 0:\n",
    "        break\n",
    "\n",
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        #print (inputs.shape)\n",
    "        # Clears the gradients of all optimized tensors\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        #print outputs.shape, labels.shape\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:\n",
    "            print ('[%d, %5d] loss: %.3f'%\n",
    "                   (epoch+1, i+1, running_loss/20))\n",
    "            running_loss = 0.0\n",
    "    print ('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "net_path = './mask_visual_infer.pth'\n",
    "torch.save(net.state_dict(),net_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(net_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing progress:  2%\n",
      "Accuracy of position -4 : 100 % and the total number is 1\n",
      "Accuracy of position -3 : 100 % and the total number is 7\n",
      "Accuracy of position -2 : 100 % and the total number is 5\n",
      "Accuracy of position -1 : 100 % and the total number is 3\n",
      "Accuracy of position  0 : 100 % and the total number is 9\n",
      "Accuracy of position  1 : 100 % and the total number is 15\n",
      "Accuracy of position  2 : 100 % and the total number is 16\n",
      "Accuracy of position  3 : 100 % and the total number is 20\n",
      "Accuracy of position 4 : 0 %\n",
      "The true accuracy of the network is 98 %\n",
      "The approximate accuracy of the network is 100 %\n"
     ]
    }
   ],
   "source": [
    "true_correct = 0\n",
    "approximate_correct = 0\n",
    "total = 0\n",
    "scope = 1\n",
    "loc_map = range(-4,5)\n",
    "position_correct = list(0 for i in range(len(loc_map)))\n",
    "position_total = list(0 for i in range(len(loc_map)))\n",
    "progress = 0.0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "         progress += 1\n",
    "         #print progress,len(test_loader)\n",
    "         if progress == 20:\n",
    "            print ('Testing progress: %2d%%'%(progress/(len(test_loader))*100))\n",
    "            break\n",
    "         test_inputs, test_labels = data\n",
    "         test_outputs = net(test_inputs)\n",
    "         _,predicted = torch.max(test_outputs.data,1)\n",
    "         #print(predicted, test_label)\n",
    "         total += test_labels.size(0)\n",
    "         true_correct += (predicted == test_labels).sum().item()\n",
    "         c = [0 for i in range(4)]\n",
    "         for i in range(test_labels.size(0)):\n",
    "             if test_labels[i]-scope <= predicted[i] <= test_labels[i]+scope:\n",
    "                 position_correct[predicted[i]] += 1\n",
    "             position_total[predicted[i]] += 1\n",
    "for i in range(len(position_total)):\n",
    "    approximate_correct += position_correct[i]\n",
    "    if position_total[i] == 0:\n",
    "        print('Accuracy of position %d : %d %%'%(loc_map[i], 0))\n",
    "    else:\n",
    "        print('Accuracy of position %2s : %d %% and the total number is %d'%(loc_map[i], 100 * position_correct[i]/position_total[i],position_total[i]))\n",
    "print( 'The true accuracy of the network is %d %%'%(100 * true_correct/total))\n",
    "print( 'The approximate accuracy of the network is %d %%'%(100 * approximate_correct/total))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "0% Testing\n",
      "       Unnamed: 0  elbow_angle  elbow_x  elbow_y  frame_idx  hand_x  hand_y  \\\n",
      "0               0       113.81     0.44     2.97          0   -1.25    2.97   \n",
      "1               1       113.81     0.29     2.99          1   -1.46    2.99   \n",
      "2               2       113.81     0.44     2.97          2   -1.25    2.97   \n",
      "3               3       116.81     0.44     2.97          3   -1.19    2.97   \n",
      "4               4       113.81     0.44     2.97          4   -1.25    2.97   \n",
      "5               5       113.81     0.60     2.94          5   -1.03    2.94   \n",
      "6               6       113.81     0.44     2.97          6   -1.25    2.97   \n",
      "7               7       110.81     0.44     2.97          7   -1.30    2.97   \n",
      "8               8       107.81     0.44     2.97          8   -1.35    2.97   \n",
      "9               9       104.81     0.44     2.97          9   -1.39    2.97   \n",
      "10             10       104.81     0.29     2.99         10   -1.59    2.99   \n",
      "11             11       101.81     0.29     2.99         11   -1.62    2.99   \n",
      "12             12       104.81     0.29     2.99         12   -1.59    2.99   \n",
      "13             13       101.81     0.29     2.99         13   -1.62    2.99   \n",
      "14             14        98.81     0.29     2.99         14   -1.65    2.99   \n",
      "15             15       101.81     0.29     2.99         15   -1.62    2.99   \n",
      "16             16       101.81     0.44     2.97         16   -1.43    2.97   \n",
      "17             17       101.81     0.29     2.99         17   -1.62    2.99   \n",
      "18             18        98.81     0.29     2.99         18   -1.65    2.99   \n",
      "19             19       101.81     0.29     2.99         19   -1.62    2.99   \n",
      "20             20       101.81     0.13     3.00         20   -1.81    3.00   \n",
      "21             21       101.81    -0.03     3.00         21   -1.99    3.00   \n",
      "22             22       101.81     0.13     3.00         22   -1.81    3.00   \n",
      "23             23       101.81    -0.03     3.00         23   -1.99    3.00   \n",
      "24             24       101.81    -0.18     2.99         24   -2.16    2.99   \n",
      "25             25        98.81    -0.18     2.99         25   -2.17    2.99   \n",
      "26             26        98.81    -0.34     2.98         26   -2.34    2.98   \n",
      "27             27        55.62    -1.03     2.82          0   -2.19    2.82   \n",
      "28             28        58.62    -1.03     2.82          1   -2.27    2.82   \n",
      "29             29        58.62    -1.17     2.76          2   -2.34    2.76   \n",
      "...           ...          ...      ...      ...        ...     ...     ...   \n",
      "10688       10688        87.01    -1.88     2.33        129   -3.37    2.33   \n",
      "10689       10689        87.01    -2.00     2.23        130   -3.42    2.23   \n",
      "10690       10690        87.01    -2.12     2.12        131   -3.46    2.12   \n",
      "10691       10691        84.01    -2.12     2.12        132   -3.38    2.12   \n",
      "10692       10692        87.01    -2.12     2.12        133   -3.46    2.12   \n",
      "10693       10693        87.01    -2.00     2.23        134   -3.42    2.23   \n",
      "10694       10694        87.01    -2.12     2.12        135   -3.46    2.12   \n",
      "10695       10695        90.01    -2.12     2.12        136   -3.53    2.12   \n",
      "10696       10696        87.01    -2.12     2.12        137   -3.46    2.12   \n",
      "10697       10697        87.01    -2.00     2.23        138   -3.42    2.23   \n",
      "10698       10698        90.01    -2.00     2.23        139   -3.49    2.23   \n",
      "10699       10699        93.01    -2.00     2.23        140   -3.56    2.23   \n",
      "10700       10700        93.01    -1.88     2.33        141   -3.50    2.33   \n",
      "10701       10701        93.01    -1.76     2.43        142   -3.44    2.43   \n",
      "10702       10702        93.01    -1.63     2.52        143   -3.36    2.52   \n",
      "10703       10703        93.01    -1.50     2.60        144   -3.28    2.60   \n",
      "10704       10704        90.01    -1.50     2.60        145   -3.23    2.60   \n",
      "10705       10705        93.01    -1.50     2.60        146   -3.28    2.60   \n",
      "10706       10706        90.01    -1.50     2.60        147   -3.23    2.60   \n",
      "10707       10707        90.01    -1.36     2.68        148   -3.14    2.68   \n",
      "10708       10708        90.01    -1.22     2.74        149   -3.04    2.74   \n",
      "10709       10709        87.01    -1.22     2.74        150   -3.00    2.74   \n",
      "10710       10710        87.01    -1.07     2.80        151   -2.90    2.80   \n",
      "10711       10711        87.01    -0.92     2.85        152   -2.79    2.85   \n",
      "10712       10712        87.01    -0.77     2.90        153   -2.67    2.90   \n",
      "10713       10713        87.01    -0.62     2.94        154   -2.55    2.94   \n",
      "10714       10714        87.01    -0.46     2.96        155   -2.42    2.96   \n",
      "10715       10715        87.01    -0.31     2.98        156   -2.28    2.98   \n",
      "10716       10716        87.01    -0.15     3.00        157   -2.14    3.00   \n",
      "10717       10717        87.01     0.01     3.00        158   -1.99    3.00   \n",
      "\n",
      "       shoulder_angle  target_x  video_idx  label  label1  \\\n",
      "0               81.50     -2.69          0      3      -1   \n",
      "1               84.50     -2.69          0      3      -1   \n",
      "2               81.50     -2.69          0      3      -1   \n",
      "3               81.50     -2.69          0      3      -1   \n",
      "4               81.50     -2.69          0      3      -1   \n",
      "5               78.50     -2.69          0      3      -1   \n",
      "6               81.50     -2.69          0      3      -1   \n",
      "7               81.50     -2.69          0      3      -1   \n",
      "8               81.50     -2.69          0      3      -1   \n",
      "9               81.50     -2.69          0      3      -1   \n",
      "10              84.50     -2.69          0      2      -2   \n",
      "11              84.50     -2.69          0      2      -2   \n",
      "12              84.50     -2.69          0      2      -2   \n",
      "13              84.50     -2.69          0      2      -2   \n",
      "14              84.50     -2.69          0      2      -2   \n",
      "15              84.50     -2.69          0      2      -2   \n",
      "16              81.50     -2.69          0      3      -1   \n",
      "17              84.50     -2.69          0      2      -2   \n",
      "18              84.50     -2.69          0      2      -2   \n",
      "19              84.50     -2.69          0      2      -2   \n",
      "20              87.50     -2.69          0      2      -2   \n",
      "21              90.50     -2.69          0      2      -2   \n",
      "22              87.50     -2.69          0      2      -2   \n",
      "23              90.50     -2.69          0      2      -2   \n",
      "24              93.50     -2.69          0      2      -2   \n",
      "25              93.50     -2.69          0      2      -2   \n",
      "26              96.50     -2.69          0      2      -2   \n",
      "27             109.99      0.19          1      2      -2   \n",
      "28             109.99      0.19          1      2      -2   \n",
      "29             112.99      0.19          1      2      -2   \n",
      "...               ...       ...        ...    ...     ...   \n",
      "10688          128.90     -1.56         49      1      -3   \n",
      "10689          131.90     -1.56         49      1      -3   \n",
      "10690          134.90     -1.56         49      1      -3   \n",
      "10691          134.90     -1.56         49      1      -3   \n",
      "10692          134.90     -1.56         49      1      -3   \n",
      "10693          131.90     -1.56         49      1      -3   \n",
      "10694          134.90     -1.56         49      1      -3   \n",
      "10695          134.90     -1.56         49      0      -4   \n",
      "10696          134.90     -1.56         49      1      -3   \n",
      "10697          131.90     -1.56         49      1      -3   \n",
      "10698          131.90     -1.56         49      1      -3   \n",
      "10699          131.90     -1.56         49      0      -4   \n",
      "10700          128.90     -1.56         49      0      -4   \n",
      "10701          125.90     -1.56         49      1      -3   \n",
      "10702          122.90     -1.56         49      1      -3   \n",
      "10703          119.90     -1.56         49      1      -3   \n",
      "10704          119.90     -1.56         49      1      -3   \n",
      "10705          119.90     -1.56         49      1      -3   \n",
      "10706          119.90     -1.56         49      1      -3   \n",
      "10707          116.90     -1.56         49      1      -3   \n",
      "10708          113.90     -1.56         49      1      -3   \n",
      "10709          113.90     -1.56         49      1      -3   \n",
      "10710          110.90     -1.56         49      1      -3   \n",
      "10711          107.90     -1.56         49      1      -3   \n",
      "10712          104.90     -1.56         49      1      -3   \n",
      "10713          101.90     -1.56         49      1      -3   \n",
      "10714           98.90     -1.56         49      2      -2   \n",
      "10715           95.90     -1.56         49      2      -2   \n",
      "10716           92.90     -1.56         49      2      -2   \n",
      "10717           89.90     -1.56         49      2      -2   \n",
      "\n",
      "                                           visualpredict  \n",
      "0      -1.68447399139 -1.7273247242 4.60711240768 11....  \n",
      "1      -1.64950811863 -0.681275844574 5.73338270187 8...  \n",
      "2      -1.68447399139 -1.7273247242 4.60711240768 11....  \n",
      "3      -1.65649044514 -1.89894890785 4.25383472443 11...  \n",
      "4      -1.68447399139 -1.7273247242 4.60711240768 11....  \n",
      "5      -1.86855876446 -2.68878865242 3.94853281975 13...  \n",
      "6      -1.68447399139 -1.7273247242 4.60711240768 11....  \n",
      "7      -1.8616091013 -1.75698566437 5.45209217072 12....  \n",
      "8      -1.85518872738 -1.71337807178 5.56006669998 12...  \n",
      "9      -1.42227375507 -1.21881878376 4.21313333511 9....  \n",
      "10     -1.67457151413 1.18181705475 8.1880979538 5.19...  \n",
      "11     -1.84794390202 1.46543180943 9.55007839203 5.7...  \n",
      "12     -1.69156241417 1.20651233196 8.29041099548 5.2...  \n",
      "13     -1.84794390202 1.46543180943 9.55007839203 5.7...  \n",
      "14     -1.71426832676 1.66301000118 9.18967533112 5.0...  \n",
      "15     -1.84794390202 1.46543180943 9.55007839203 5.7...  \n",
      "16     -1.6105992794 -0.975694298744 5.10512399673 8....  \n",
      "17     -1.84794390202 1.46543180943 9.55007839203 5.7...  \n",
      "18     -1.71426832676 1.66301000118 9.18967533112 5.0...  \n",
      "19     -1.84794390202 1.46543180943 9.55007839203 5.7...  \n",
      "20     -1.9593526125 2.72204422951 12.3787317276 5.67...  \n",
      "21     -1.88844335079 4.03601694107 10.8144569397 2.4...  \n",
      "22     -1.9593526125 2.72204422951 12.3787317276 5.67...  \n",
      "23     -1.88844335079 4.03601694107 10.8144569397 2.4...  \n",
      "24     -1.8702968359 3.80371785164 9.62898159027 1.91...  \n",
      "25     -2.01351356506 4.35019445419 10.5946331024 1.9...  \n",
      "26     -1.22552525997 4.48463153839 7.88265943527 0.8...  \n",
      "27     -1.81289219856 3.21459078789 6.76665639877 0.8...  \n",
      "28     -1.81126916409 4.41481733322 7.6654086113 0.72...  \n",
      "29     -1.37109482288 3.39196681976 5.6378993988 0.45...  \n",
      "...                                                  ...  \n",
      "10688  3.44782280922 5.52090406418 0.640782952309 -1....  \n",
      "10689  4.25046539307 4.68466997147 0.0351833999157 -1...  \n",
      "10690  5.35986042023 5.60867643356 0.00666531920433 -...  \n",
      "10691  3.93166089058 5.42972326279 0.486916571856 -1....  \n",
      "10692  5.35986042023 5.60867643356 0.00666531920433 -...  \n",
      "10693  4.25046539307 4.68466997147 0.0351833999157 -1...  \n",
      "10694  5.48357105255 5.73615980148 0.0089792907238 -2...  \n",
      "10695  4.58502912521 3.99178290367 -0.311869293451 -1...  \n",
      "10696  5.48357105255 5.73615980148 0.0089792907238 -2...  \n",
      "10697  4.25046539307 4.68466997147 0.0351833999157 -1...  \n",
      "10698  5.69209527969 5.28133487701 -0.25636318326 -2....  \n",
      "10699  5.35881280899 3.97655034065 -0.674375891685 -1...  \n",
      "10700  4.8927564621 4.53269529343 -0.24223831296 -1.7...  \n",
      "10701  4.59323453903 5.4611749649 0.0441032350063 -1....  \n",
      "10702  3.19498801231 5.41402482986 0.405247062445 -1....  \n",
      "10703  2.58083176613 6.9304728508 1.02796888351 -1.79...  \n",
      "10704  3.03929495811 8.62860012054 1.44354426861 -2.2...  \n",
      "10705  2.51446819305 6.93933916092 1.04532325268 -1.7...  \n",
      "10706  3.03929495811 8.62860012054 1.44354426861 -2.2...  \n",
      "10707  2.11720490456 6.21826601028 0.910489678383 -1....  \n",
      "10708  2.43490862846 9.60801410675 1.88346171379 -2.6...  \n",
      "10709  2.10765743256 9.04545497894 1.9682469368 -2.37...  \n",
      "10710  1.66421306133 8.28684711456 2.02313685417 -2.2...  \n",
      "10711  0.7830286026 9.63470172882 3.71679782867 -2.45...  \n",
      "10712  0.515148758888 9.35478115082 4.0950961113 -2.3...  \n",
      "10713  -0.627891659737 7.0418047905 5.85294055939 -1....  \n",
      "10714  -0.766713023186 6.3208489418 7.25295162201 -0....  \n",
      "10715  -1.57708728313 4.87703704834 9.46738815308 1.1...  \n",
      "10716  -1.62745392323 3.33642077446 8.85596179962 1.6...  \n",
      "10717  -1.88035571575 3.78040957451 11.5498790741 2.8...  \n",
      "\n",
      "[10718 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "image_dataloader = DataLoader(image_dataset,batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(image_dataloader):\n",
    "        if idx%200 == 199:\n",
    "            print ('%d%% Testing'%(idx*1.0/len(image_dataloader)*100.0))\n",
    "        image_inputs, iamge_labels = data\n",
    "        image_outputs = net(image_inputs)\n",
    "        l1 = image_outputs.tolist()\n",
    "        df.loc[idx,'visualpredict'] = ' '.join(str(i) for i in l1[0])\n",
    "print df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "df.to_csv('/Users/Jipeng/PycharmProjects/simulated_multisensory_integration/data/after_visual.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-20495502",
   "language": "python",
   "display_name": "PyCharm (dopamine_rl)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}